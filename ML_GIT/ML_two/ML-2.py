# -*- coding: utf-8 -*-
"""Assignment2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sKJHtY8wxr2PZ-Avq6XB-i5yvLBvvV0u
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer
from sklearn.model_selection import train_test_split, KFold, cross_val_score
import numpy as np
from sklearn.feature_selection import SelectKBest, chi2, f_classif
from IPython.display import display

# Naïve Bayes models
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB

# KNN models
from sklearn.neighbors import KNeighborsClassifier

# SVM models
from sklearn.svm import SVC

# Metrics
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                            f1_score, confusion_matrix, classification_report,
                            roc_curve, auc, roc_auc_score)

# 1. Loading the dataset
df = pd.read_csv("/content/spambase_csv.csv")
display(df.head())

# 2. Preprocessing the data

print("No of Instances : ",df.shape[0])
print()
print("No of Features : ",df.shape[1])
print()
print("Instances :")
for col in df.columns:
    print(col)
print()
print()
print("Number of instances/class : ",df.groupby("class").size())
print()

print("\n--- Data Types ---")
print(df.info())

print("--- Missing Values ---")
print(df.isnull().sum())

print(df.describe())

# Standardization
numeric_features = df.select_dtypes(include=['float64', 'int64']).columns.tolist()
if 'class' in numeric_features:
    numeric_features.remove('class')

# Apply MinMax Scaling
minmax = MinMaxScaler()
df[numeric_features] = minmax.fit_transform(df[numeric_features])

print("--- Data after MinMax Scaling ---")
print(df.head(5))
print()
print(df.describe())
print()

#  EXPLORATORY DATA ANALYSIS (EDA)


print(" EXPLORATORY DATA ANALYSIS")


# Class distribution
class_counts = df['class'].value_counts()
print("\nClass Distribution:")
print(f"Ham (0): {class_counts[0]} ({class_counts[0]/len(df)*100:.2f}%)")
print(f"Spam (1): {class_counts[1]} ({class_counts[1]/len(df)*100:.2f}%)")

# Visualize class distribution
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Bar plot
axes[0].bar(['Ham (0)', 'Spam (1)'], class_counts.values, color=['#10b981', '#ef4444'])
axes[0].set_title('Class Distribution', fontsize=14, fontweight='bold')
axes[0].set_ylabel('Count')
axes[0].grid(axis='y', alpha=0.3)
for i, v in enumerate(class_counts.values):
    axes[0].text(i, v + 50, str(v), ha='center', fontweight='bold')

# Pie chart
colors = ['#10b981', '#ef4444']
axes[1].pie(class_counts.values, labels=['Ham (0)', 'Spam (1)'], autopct='%1.1f%%',
           colors=colors, startangle=90, textprops={'fontsize': 12, 'fontweight': 'bold'})
axes[1].set_title('Class Distribution (%)', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

# Feature distributions for top spam indicators
spam_emails = df[df['class'] == 1]
ham_emails = df[df['class'] == 0]

print(df.describe())

X = df.drop('class',axis=1)
Y = df['class']

print(X.head)
print(Y.head)

# FEATURE SELECTION

print("\n" + "="*80)
print("STEP 2.2: FEATURE SELECTION")
print("="*80)

X = df.drop('class', axis=1)
Y = df['class']

print("Original Features:")
print(X.head())
print()
print("Target Variable:")
print(Y.head())
print()

# Feature Selection - ANOVA
selector = SelectKBest(score_func=f_classif, k=20)
X_new = selector.fit_transform(X, Y)
selected_cols = selector.get_support(indices=True)
X_df = pd.DataFrame(X_new, columns=X.columns[selected_cols])

print("Selected Features (Top 20):")
display(X_df.head(20))
print()

# Feature scores
feature_scores = selector.scores_
selected_features = X.columns[selector.get_support()]

print("ANOVA Test:\n")
print("FEATURE SCORES:")
for feat, score in zip(X.columns, feature_scores):
    print(f"  {feat}: {score:.4f}")
print()

print("Selected Features:")
print(selected_features.tolist())
print()

# Update X with selected features
X = X[selected_features]
display(X)

X = X[selected_features]

# SPLITTING THE DATA

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42, stratify=Y)

print(f"Train set size: {X_train.shape[0]} samples")
print(f"Test set size: {X_test.shape[0]} samples")
print(f"Train class distribution: {np.bincount(y_train)}")
print(f"Test class distribution: {np.bincount(y_test)}")


scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("\n✓ Data preprocessing completed!")

#  MODEL TRAINING AND EVALUATION
print(" MODEL TRAINING AND EVALUATION")

# Dictionary to store all results
results = {}

def evaluate_model(name, model, X_train_data, X_test_data, y_train, y_test):
    """Train and evaluate model"""
    # Train
    model.fit(X_train_data, y_train)

    # Predict
    y_pred = model.predict(X_test_data)

    # Get probabilities if available
    try:
        y_pred_proba = model.predict_proba(X_test_data)[:, 1]
    except:
        y_pred_proba = None

    # Calculate metrics
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    cm = confusion_matrix(y_test, y_pred)

    # Store results
    results[name] = {
        'model': model,
        'accuracy': acc,
        'precision': prec,
        'recall': rec,
        'f1_score': f1,
        'confusion_matrix': cm,
        'predictions': y_pred,
        'pred_proba': y_pred_proba
    }

    # Print results
    print(f"\n{name}")
    print("-" * 70)
    print(f"Accuracy:  {acc:.4f}")
    print(f"Precision: {prec:.4f}")
    print(f"Recall:    {rec:.4f}")
    print(f"F1-Score:  {f1:.4f}")
    print(f"Confusion Matrix:\n{cm}")

    return model, y_pred

#  NAÏVE BAYES MODELS
print(" NAÏVE BAYES MODELS")

# Gaussian Naïve Bayes
print("\n Training Gaussian Naïve Bayes...")
gnb_model = GaussianNB()
gnb_model, gnb_pred = evaluate_model('Gaussian NB', gnb_model, X_train, X_test, y_train, y_test)

# Multinomial Naïve Bayes
print("\n Training Multinomial Naïve Bayes...")
mnb_model = MultinomialNB()
mnb_model, mnb_pred = evaluate_model('Multinomial NB', mnb_model, X_train, X_test, y_train, y_test)

# Bernoulli Naïve Bayes
print("\n Training Bernoulli Naïve Bayes...")
bnb_model = BernoulliNB()
bnb_model, bnb_pred = evaluate_model('Bernoulli NB', bnb_model, X_train, X_test, y_train, y_test)

#  K-NEAREST NEIGHBORS (KNN)
print("K-NEAREST NEIGHBORS (KNN)")

# Test different K values
k_values = [3, 5, 7, 9, 11]

for k in k_values:
    print(f"\n[KNN with k={k}] Training...")
    knn_model = KNeighborsClassifier(n_neighbors=k, algorithm='auto')
    knn_model, knn_pred = evaluate_model(f'KNN (k={k})', knn_model, X_train_scaled, X_test_scaled, y_train, y_test)

# Test different algorithms with k=5
print("\n" + "-"*70)
print("Testing different KNN algorithms (k=5):")
print("-"*70)

algorithms = ['auto', 'ball_tree', 'kd_tree']

for algo in algorithms:
    print(f"\n[KNN - {algo}] Training...")
    knn_model = KNeighborsClassifier(n_neighbors=5, algorithm=algo)
    knn_model, knn_pred = evaluate_model(f'KNN ({algo})', knn_model, X_train_scaled, X_test_scaled, y_train, y_test)

#  SUPPORT VECTOR MACHINE (SVM)

print("SUPPORT VECTOR MACHINE (SVM)")

kernels = ['linear', 'poly', 'rbf', 'sigmoid']

for kernel in kernels:
    print(f"\n[SVM - {kernel} kernel] Training...")
    if kernel == 'poly':
        svm_model = SVC(kernel=kernel, degree=3, probability=True, random_state=42)
    else:
        svm_model = SVC(kernel=kernel, probability=True, random_state=42)

    svm_model, svm_pred = evaluate_model(f'SVM ({kernel})', svm_model, X_train_scaled, X_test_scaled, y_train, y_test)

#  CONFUSION MATRICES VISUALIZATION
print("CONFUSION MATRICES")


#  key models for visualization
key_models = [
    'Gaussian NB', 'Multinomial NB', 'Bernoulli NB',
    'KNN (k=3)', 'KNN (k=5)', 'KNN (k=7)',
    'SVM (linear)', 'SVM (rbf)', 'SVM (poly)', 'SVM (sigmoid)'
]

fig, axes = plt.subplots(2, 5, figsize=(20, 8))
axes = axes.ravel()

for idx, model_name in enumerate(key_models):
    if model_name in results:
        cm = results[model_name]['confusion_matrix']
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],
                   cbar=False, square=True, linewidths=1,
                   xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])
        axes[idx].set_title(f'{model_name}\nAcc: {results[model_name]["accuracy"]:.3f}',
                           fontweight='bold', fontsize=10)
        axes[idx].set_xlabel('Predicted')
        axes[idx].set_ylabel('Actual')

plt.suptitle('Confusion Matrices - All Models', fontsize=16, fontweight='bold', y=1.02)
plt.tight_layout()
plt.show()

#  ROC CURVES

# Plot ROC curves for all models
plt.figure(figsize=(14, 10))

colors = plt.cm.tab20(np.linspace(0, 1, len(results)))

for idx, (model_name, result) in enumerate(results.items()):
    if result['pred_proba'] is not None:
        fpr, tpr, _ = roc_curve(y_test, result['pred_proba'])
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.3f})',
                linewidth=2, color=colors[idx])

plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')
plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')
plt.title('ROC Curves - All Models', fontsize=14, fontweight='bold')
plt.legend(loc='lower right', fontsize=9)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# Separate ROC curves by model type
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Naïve Bayes
for model_name in ['Gaussian NB', 'Multinomial NB', 'Bernoulli NB']:
    if model_name in results and results[model_name]['pred_proba'] is not None:
        fpr, tpr, _ = roc_curve(y_test, results[model_name]['pred_proba'])
        roc_auc = auc(fpr, tpr)
        axes[0].plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.3f})', linewidth=2)

axes[0].plot([0, 1], [0, 1], 'k--', linewidth=2)
axes[0].set_xlabel('False Positive Rate', fontweight='bold')
axes[0].set_ylabel('True Positive Rate', fontweight='bold')
axes[0].set_title('Naïve Bayes Models', fontsize=12, fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

# KNN
for k in [3, 5, 7, 9, 11]:
    model_name = f'KNN (k={k})'
    if model_name in results and results[model_name]['pred_proba'] is not None:
        fpr, tpr, _ = roc_curve(y_test, results[model_name]['pred_proba'])
        roc_auc = auc(fpr, tpr)
        axes[1].plot(fpr, tpr, label=f'k={k} (AUC = {roc_auc:.3f})', linewidth=2)

axes[1].plot([0, 1], [0, 1], 'k--', linewidth=2)
axes[1].set_xlabel('False Positive Rate', fontweight='bold')
axes[1].set_ylabel('True Positive Rate', fontweight='bold')
axes[1].set_title('KNN Models', fontsize=12, fontweight='bold')
axes[1].legend()
axes[1].grid(alpha=0.3)

# SVM
for kernel in ['linear', 'poly', 'rbf', 'sigmoid']:
    model_name = f'SVM ({kernel})'
    if model_name in results and results[model_name]['pred_proba'] is not None:
        fpr, tpr, _ = roc_curve(y_test, results[model_name]['pred_proba'])
        roc_auc = auc(fpr, tpr)
        axes[2].plot(fpr, tpr, label=f'{kernel} (AUC = {roc_auc:.3f})', linewidth=2)

axes[2].plot([0, 1], [0, 1], 'k--', linewidth=2)
axes[2].set_xlabel('False Positive Rate', fontweight='bold')
axes[2].set_ylabel('True Positive Rate', fontweight='bold')
axes[2].set_title('SVM Models', fontsize=12, fontweight='bold')
axes[2].legend()
axes[2].grid(alpha=0.3)

plt.tight_layout()
plt.show()

#  K-FOLD CROSS VALIDATION (K=5)

print(" K-FOLD CROSS VALIDATION (K=5)")

kfold = KFold(n_splits=5, shuffle=True, random_state=42)
cv_results = {}

# Naïve Bayes
print("\nNaïve Bayes Models:")
print("-" * 70)
for name, model_class in [('Gaussian NB', GaussianNB()),
                          ('Multinomial NB', MultinomialNB()),
                          ('Bernoulli NB', BernoulliNB())]:
    scores = cross_val_score(model_class, X_train, y_train, cv=kfold, scoring='accuracy')
    cv_results[name] = scores
    print(f"{name:20s} - Mean: {scores.mean():.4f} (+/- {scores.std():.4f})")
    print(f"{'':20s}   Fold Scores: {[f'{s:.4f}' for s in scores]}")

# KNN
print("\nKNN Models:")
print("-" * 70)
for k in [3, 5, 7, 9, 11]:
    model = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(model, X_train_scaled, y_train, cv=kfold, scoring='accuracy')
    name = f'KNN (k={k})'
    cv_results[name] = scores
    print(f"{name:20s} - Mean: {scores.mean():.4f} (+/- {scores.std():.4f})")
    print(f"{'':20s}   Fold Scores: {[f'{s:.4f}' for s in scores]}")

# SVM
print("\nSVM Models:")
print("-" * 70)
for kernel in ['linear', 'rbf', 'poly', 'sigmoid']:
    if kernel == 'poly':
        model = SVC(kernel=kernel, degree=3, random_state=42)
    else:
        model = SVC(kernel=kernel, random_state=42)
    scores = cross_val_score(model, X_train_scaled, y_train, cv=kfold, scoring='accuracy')
    name = f'SVM ({kernel})'
    cv_results[name] = scores
    print(f"{name:20s} - Mean: {scores.mean():.4f} (+/- {scores.std():.4f})")
    print(f"{'':20s}   Fold Scores: {[f'{s:.4f}' for s in scores]}")

# Visualize cross-validation results
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Naïve Bayes
nb_models = ['Gaussian NB', 'Multinomial NB', 'Bernoulli NB']
nb_scores = [cv_results[m] for m in nb_models]
bp1 = axes[0].boxplot(nb_scores, labels=nb_models, patch_artist=True)
for patch in bp1['boxes']:
    patch.set_facecolor('#3b82f6')
axes[0].set_title('Naïve Bayes - 5-Fold CV', fontsize=12, fontweight='bold')
axes[0].set_ylabel('Accuracy', fontweight='bold')
axes[0].grid(axis='y', alpha=0.3)
axes[0].tick_params(axis='x', rotation=15)

# KNN
knn_models = [f'KNN (k={k})' for k in [3, 5, 7, 9, 11]]
knn_scores = [cv_results[m] for m in knn_models]
bp2 = axes[1].boxplot(knn_scores, labels=[f'k={k}' for k in [3, 5, 7, 9, 11]],
                       patch_artist=True)
for patch in bp2['boxes']:
    patch.set_facecolor('#10b981')
axes[1].set_title('KNN - 5-Fold CV', fontsize=12, fontweight='bold')
axes[1].set_ylabel('Accuracy', fontweight='bold')
axes[1].grid(axis='y', alpha=0.3)

# SVM
svm_models = [f'SVM ({k})' for k in ['linear', 'rbf', 'poly', 'sigmoid']]
svm_scores = [cv_results[m] for m in svm_models]
bp3 = axes[2].boxplot(svm_scores, labels=['linear', 'rbf', 'poly', 'sigmoid'],
                       patch_artist=True)
for patch in bp3['boxes']:
    patch.set_facecolor('#f59e0b')
axes[2].set_title('SVM - 5-Fold CV', fontsize=12, fontweight='bold')
axes[2].set_ylabel('Accuracy', fontweight='bold')
axes[2].grid(axis='y', alpha=0.3)
axes[2].tick_params(axis='x', rotation=15)

plt.tight_layout()
plt.show()

# COMPREHENSIVE MODEL COMPARISON

# Create comparison dataframe
comparison_data = []
for model_name, result in results.items():
    comparison_data.append({
        'Model': model_name,
        'Accuracy': result['accuracy'],
        'Precision': result['precision'],
        'Recall': result['recall'],
        'F1-Score': result['f1_score']
    })

comparison_df = pd.DataFrame(comparison_data)
comparison_df = comparison_df.sort_values('F1-Score', ascending=False)

print("\nTest Set Performance (sorted by F1-Score):")
print("=" * 80)
print(comparison_df.to_string(index=False))

# Visualize comparison
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
colors_map = {'Accuracy': '#3b82f6', 'Precision': '#10b981',
              'Recall': '#f59e0b', 'F1-Score': '#ef4444'}

for idx, metric in enumerate(metrics):
    ax = axes[idx // 2, idx % 2]
    top_10 = comparison_df.nlargest(10, metric)

    bars = ax.barh(range(len(top_10)), top_10[metric].values, color=colors_map[metric])
    ax.set_yticks(range(len(top_10)))
    ax.set_yticklabels(top_10['Model'].values)
    ax.set_xlabel(metric, fontweight='bold')
    ax.set_title(f'Top 10 Models by {metric}', fontsize=12, fontweight='bold')
    ax.grid(axis='x', alpha=0.3)
    ax.invert_yaxis()

    # Add value labels
    for i, (bar, val) in enumerate(zip(bars, top_10[metric].values)):
        ax.text(val + 0.01, i, f'{val:.3f}', va='center', fontweight='bold')

plt.tight_layout()
plt.show()

# Model type comparison
model_types = {
    'Naïve Bayes': [m for m in results.keys() if 'NB' in m],
    'KNN': [m for m in results.keys() if 'KNN' in m and 'k=' in m],
    'SVM': [m for m in results.keys() if 'SVM' in m]
}

type_comparison = []
for model_type, models in model_types.items():
    if models:
        type_metrics = {
            'Accuracy': np.mean([results[m]['accuracy'] for m in models]),
            'Precision': np.mean([results[m]['precision'] for m in models]),
            'Recall': np.mean([results[m]['recall'] for m in models]),
            'F1-Score': np.mean([results[m]['f1_score'] for m in models])
        }
        type_comparison.append({'Model Type': model_type, **type_metrics})

type_df = pd.DataFrame(type_comparison)

print("\n" + "=" * 80)
print("Average Performance by Model Type:")
print("=" * 80)
print(type_df.to_string(index=False))

# Radar chart for model type comparison
from math import pi

fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))

angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
angles += angles[:1]

colors_radar = ['#3b82f6', '#10b981', '#f59e0b']

for idx, row in type_df.iterrows():
    values = [row[m] for m in metrics]
    values += values[:1]
    ax.plot(angles, values, 'o-', linewidth=2, label=row['Model Type'], color=colors_radar[idx])
    ax.fill(angles, values, alpha=0.15, color=colors_radar[idx])

ax.set_xticks(angles[:-1])
ax.set_xticklabels(metrics, fontweight='bold')
ax.set_ylim(0, 1)
ax.set_title('Model Type Comparison - All Metrics', fontsize=14, fontweight='bold', pad=20)
ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))
ax.grid(True)

plt.tight_layout()
plt.show()

#  FINAL OBSERVATIONS AND RECOMMENDATIONS

best_model = comparison_df.iloc[0]
print(f"\n BEST OVERALL MODEL: {best_model['Model']}")
print("-" * 80)
print(f"   Accuracy:  {best_model['Accuracy']:.4f}")
print(f"   Precision: {best_model['Precision']:.4f}")
print(f"   Recall:    {best_model['Recall']:.4f}")
print(f"   F1-Score:  {best_model['F1-Score']:.4f}")

print("\n KEY OBSERVATIONS:")
print("-" * 80)

# Best in each category
for metric in metrics:
    best = comparison_df.nlargest(1, metric).iloc[0]
    print(f"   Best {metric:12s}: {best['Model']:25s} ({best[metric]:.4f})")

# ---------- Table 1: Naïve Bayes Variant Comparison ----------
nb_table = pd.DataFrame({
    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],
    'Gaussian NB': [
        results['Gaussian NB']['accuracy'],
        results['Gaussian NB']['precision'],
        results['Gaussian NB']['recall'],
        results['Gaussian NB']['f1_score']
    ],
    'Multinomial NB': [
        results['Multinomial NB']['accuracy'],
        results['Multinomial NB']['precision'],
        results['Multinomial NB']['recall'],
        results['Multinomial NB']['f1_score']
    ],
    'Bernoulli NB': [
        results['Bernoulli NB']['accuracy'],
        results['Bernoulli NB']['precision'],
        results['Bernoulli NB']['recall'],
        results['Bernoulli NB']['f1_score']
    ]
})

display(nb_table.round(4))


# ---------- Table 2: KNN – Varying K Values ----------
k_values = [3, 5, 7, 9]   # only trained K values

knn_k_df = pd.DataFrame([
    [k,
     results[f'KNN (k={k})']['accuracy'],
     results[f'KNN (k={k})']['precision'],
     results[f'KNN (k={k})']['recall'],
     results[f'KNN (k={k})']['f1_score']]
    for k in k_values
], columns=['k', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])

display(knn_k_df.round(4))


# ---------- Table 3: KNN – KDTree vs BallTree ----------
knn_tree_df = pd.DataFrame({
    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],
    'KDTree': [
        results['KNN (kd_tree)']['accuracy'],
        results['KNN (kd_tree)']['precision'],
        results['KNN (kd_tree)']['recall'],
        results['KNN (kd_tree)']['f1_score']
    ],
    'BallTree': [
        results['KNN (ball_tree)']['accuracy'],
        results['KNN (ball_tree)']['precision'],
        results['KNN (ball_tree)']['recall'],
        results['KNN (ball_tree)']['f1_score']
    ]
})

display(knn_tree_df.round(4))


# ---------- Table 4: SVM Kernel-wise Results ----------
svm_df = pd.DataFrame([
    [kernel,
     results[f'SVM ({kernel})']['accuracy'],
     results[f'SVM ({kernel})']['f1_score']]
    for kernel in ['linear', 'poly', 'rbf', 'sigmoid']
], columns=['Kernel', 'Accuracy', 'F1 Score'])

display(svm_df.round(4))


# ---------- Table 5: 5-Fold Cross Validation Results ----------
cv_df = pd.DataFrame({
    'Fold': [f'Fold {i+1}' for i in range(5)] + ['Average'],
    'Naïve Bayes Accuracy': list(cv_results['Gaussian NB']) + [cv_results['Gaussian NB'].mean()],
    'KNN Accuracy': list(cv_results['KNN (k=5)']) + [cv_results['KNN (k=5)'].mean()],
    'SVM Accuracy': list(cv_results['SVM (rbf)']) + [cv_results['SVM (rbf)'].mean()]
})

display(cv_df.round(4))

# -*- coding: utf-8 -*-
"""Assignment3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AvVVaoZlveuhPYk3M9ekeU8nfmcDe8rS
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer
from sklearn.model_selection import train_test_split, KFold, cross_val_score
import numpy as np
from sklearn.feature_selection import SelectKBest, chi2, f_classif

# Naïve Bayes models
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB

# KNN models
from sklearn.neighbors import KNeighborsClassifier

# SVM models
from sklearn.svm import SVC

# Metrics
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                            f1_score, confusion_matrix, classification_report,
                            roc_curve, auc, roc_auc_score)

# 1. Loading the dataset
df = pd.read_csv("/content/spambase_csv.csv")
print(df.head())

# 2. Preprocessing the data

print("No of Instances : ",df.shape[0])
print()
print("No of Features : ",df.shape[1])
print()
print("Instances :")
for col in df.columns:
    print(col)
print()
print()
print("Number of instances/class : ",df.groupby("class").size())
print()

print("\n--- Data Types ---")
print(df.info())

print("--- Missing Values ---")
print(df.isnull().sum())

print(df.describe())

# Standardization
numeric_features = df.select_dtypes(include=['float64', 'int64']).columns.tolist()
if 'class' in numeric_features:
    numeric_features.remove('class')

# Apply MinMax Scaling
minmax = MinMaxScaler()
df[numeric_features] = minmax.fit_transform(df[numeric_features])

print("--- Data after MinMax Scaling ---")
print(df.head(5))
print()
print(df.describe())
print()

#  EXPLORATORY DATA ANALYSIS (EDA)


print(" EXPLORATORY DATA ANALYSIS")


# Class distribution
class_counts = df['class'].value_counts()
print("\nClass Distribution:")
print(f"Ham (0): {class_counts[0]} ({class_counts[0]/len(df)*100:.2f}%)")
print(f"Spam (1): {class_counts[1]} ({class_counts[1]/len(df)*100:.2f}%)")

# Visualize class distribution
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Bar plot
axes[0].bar(['Ham (0)', 'Spam (1)'], class_counts.values, color=['#10b981', '#ef4444'])
axes[0].set_title('Class Distribution', fontsize=14, fontweight='bold')
axes[0].set_ylabel('Count')
axes[0].grid(axis='y', alpha=0.3)
for i, v in enumerate(class_counts.values):
    axes[0].text(i, v + 50, str(v), ha='center', fontweight='bold')

# Pie chart
colors = ['#10b981', '#ef4444']
axes[1].pie(class_counts.values, labels=['Ham (0)', 'Spam (1)'], autopct='%1.1f%%',
           colors=colors, startangle=90, textprops={'fontsize': 12, 'fontweight': 'bold'})
axes[1].set_title('Class Distribution (%)', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

# Feature distributions for top spam indicators
spam_emails = df[df['class'] == 1]
ham_emails = df[df['class'] == 0]

print(df.describe())

X = df.drop('class',axis=1)
Y = df['class']

print(X.head())
print(Y.head())

# FEATURE SELECTION

print("\n" + "="*80)
print("STEP 2.2: FEATURE SELECTION")
print("="*80)

X = df.drop('class', axis=1)
Y = df['class']

print("Original Features:")
print(X.head())
print()
print("Target Variable:")
print(Y.head())
print()

# Feature Selection - ANOVA
selector = SelectKBest(score_func=f_classif, k=20)
X_new = selector.fit_transform(X, Y)
selected_cols = selector.get_support(indices=True)
X_df = pd.DataFrame(X_new, columns=X.columns[selected_cols])

print("Selected Features (Top 20):")
print(X_df.head(20))
print()

# Feature scores
feature_scores = selector.scores_
selected_features = X.columns[selector.get_support()]

print("ANOVA Test:\n")
print("FEATURE SCORES:")
for feat, score in zip(X.columns, feature_scores):
    print(f"  {feat}: {score:.4f}")
print()

print("Selected Features:")
print(selected_features.tolist())
print()

# Update X with selected features
X = X[selected_features]
print(X)

X = X[selected_features]

# SPLITTING THE DATA

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42, stratify=Y)

print(f"Train set size: {X_train.shape[0]} samples")
print(f"Test set size: {X_test.shape[0]} samples")
print(f"Train class distribution: {np.bincount(y_train)}")
print(f"Test class distribution: {np.bincount(y_test)}")


scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("\n✓ Data preprocessing completed!")

#  MODEL TRAINING AND EVALUATION
print(" MODEL TRAINING AND EVALUATION")

# Dictionary to store all results
results = {}

def evaluate_model(name, model, X_train_data, X_test_data, y_train, y_test):
    """Train and evaluate model"""
    # Train
    model.fit(X_train_data, y_train)

    # Predict
    y_pred = model.predict(X_test_data)

    # Get probabilities if available
    try:
        y_pred_proba = model.predict_proba(X_test_data)[:, 1]
    except:
        y_pred_proba = None

    # Calculate metrics
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    cm = confusion_matrix(y_test, y_pred)

    # Store results
    results[name] = {
        'model': model,
        'accuracy': acc,
        'precision': prec,
        'recall': rec,
        'f1_score': f1,
        'confusion_matrix': cm,
        'predictions': y_pred,
        'pred_proba': y_pred_proba
    }

    # Print results
    print(f"\n{name}")
    print("-" * 70)
    print(f"Accuracy:  {acc:.4f}")
    print(f"Precision: {prec:.4f}")
    print(f"Recall:    {rec:.4f}")
    print(f"F1-Score:  {f1:.4f}")
    print(f"Confusion Matrix:\n{cm}")

    return model, y_pred

#  NAÏVE BAYES MODELS
print(" NAÏVE BAYES MODELS")

# Gaussian Naïve Bayes
print("\n Training Gaussian Naïve Bayes...")
gnb_model = GaussianNB()
gnb_model, gnb_pred = evaluate_model('Gaussian NB', gnb_model, X_train, X_test, y_train, y_test)

# Multinomial Naïve Bayes
print("\n Training Multinomial Naïve Bayes...")
mnb_model = MultinomialNB()
mnb_model, mnb_pred = evaluate_model('Multinomial NB', mnb_model, X_train, X_test, y_train, y_test)

# Bernoulli Naïve Bayes
print("\n Training Bernoulli Naïve Bayes...")
bnb_model = BernoulliNB()
bnb_model, bnb_pred = evaluate_model('Bernoulli NB', bnb_model, X_train, X_test, y_train, y_test)

#  K-NEAREST NEIGHBORS (KNN)
print("K-NEAREST NEIGHBORS (KNN)")

# Test different K values
k_values = [3, 5, 7, 9, 11]

for k in k_values:
    print(f"\n[KNN with k={k}] Training...")
    knn_model = KNeighborsClassifier(n_neighbors=k, algorithm='auto')
    knn_model, knn_pred = evaluate_model(f'KNN (k={k})', knn_model, X_train_scaled, X_test_scaled, y_train, y_test)

# Test different algorithms with k=5
print("\n" + "-"*70)
print("Testing different KNN algorithms (k=5):")
print("-"*70)

algorithms = ['auto', 'ball_tree', 'kd_tree']

for algo in algorithms:
    print(f"\n[KNN - {algo}] Training...")
    knn_model = KNeighborsClassifier(n_neighbors=5, algorithm=algo)
    knn_model, knn_pred = evaluate_model(f'KNN ({algo})', knn_model, X_train_scaled, X_test_scaled, y_train, y_test)

#  SUPPORT VECTOR MACHINE (SVM)

print("SUPPORT VECTOR MACHINE (SVM)")

kernels = ['linear', 'poly', 'rbf', 'sigmoid']

for kernel in kernels:
    print(f"\n[SVM - {kernel} kernel] Training...")
    if kernel == 'poly':
        svm_model = SVC(kernel=kernel, degree=3, probability=True, random_state=42)
    else:
        svm_model = SVC(kernel=kernel, probability=True, random_state=42)

    svm_model, svm_pred = evaluate_model(f'SVM ({kernel})', svm_model, X_train_scaled, X_test_scaled, y_train, y_test)

#  CONFUSION MATRICES VISUALIZATION
print("CONFUSION MATRICES")


#  key models for visualization
key_models = [
    'Gaussian NB', 'Multinomial NB', 'Bernoulli NB',
    'KNN (k=3)', 'KNN (k=5)', 'KNN (k=7)',
    'SVM (linear)', 'SVM (rbf)', 'SVM (poly)', 'SVM (sigmoid)'
]

fig, axes = plt.subplots(2, 5, figsize=(20, 8))
axes = axes.ravel()

for idx, model_name in enumerate(key_models):
    if model_name in results:
        cm = results[model_name]['confusion_matrix']
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],
                   cbar=False, square=True, linewidths=1,
                   xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])
        axes[idx].set_title(f'{model_name}\nAcc: {results[model_name]["accuracy"]:.3f}',
                           fontweight='bold', fontsize=10)
        axes[idx].set_xlabel('Predicted')
        axes[idx].set_ylabel('Actual')

plt.suptitle('Confusion Matrices - All Models', fontsize=16, fontweight='bold', y=1.02)
plt.tight_layout()
plt.show()

#  ROC CURVES

# Plot ROC curves for all models
plt.figure(figsize=(14, 10))

colors = plt.cm.tab20(np.linspace(0, 1, len(results)))

for idx, (model_name, result) in enumerate(results.items()):
    if result['pred_proba'] is not None:
        fpr, tpr, _ = roc_curve(y_test, result['pred_proba'])
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.3f})',
                linewidth=2, color=colors[idx])

plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')
plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')
plt.title('ROC Curves - All Models', fontsize=14, fontweight='bold')
plt.legend(loc='lower right', fontsize=9)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# Separate ROC curves by model type
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Naïve Bayes
for model_name in ['Gaussian NB', 'Multinomial NB', 'Bernoulli NB']:
    if model_name in results and results[model_name]['pred_proba'] is not None:
        fpr, tpr, _ = roc_curve(y_test, results[model_name]['pred_proba'])
        roc_auc = auc(fpr, tpr)
        axes[0].plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.3f})', linewidth=2)

axes[0].plot([0, 1], [0, 1], 'k--', linewidth=2)
axes[0].set_xlabel('False Positive Rate', fontweight='bold')
axes[0].set_ylabel('True Positive Rate', fontweight='bold')
axes[0].set_title('Naïve Bayes Models', fontsize=12, fontweight='bold')
axes[0].legend()
axes[0].grid(alpha=0.3)

# KNN
for k in [3, 5, 7, 9, 11]:
    model_name = f'KNN (k={k})'
    if model_name in results and results[model_name]['pred_proba'] is not None:
        fpr, tpr, _ = roc_curve(y_test, results[model_name]['pred_proba'])
        roc_auc = auc(fpr, tpr)
        axes[1].plot(fpr, tpr, label=f'k={k} (AUC = {roc_auc:.3f})', linewidth=2)

axes[1].plot([0, 1], [0, 1], 'k--', linewidth=2)
axes[1].set_xlabel('False Positive Rate', fontweight='bold')
axes[1].set_ylabel('True Positive Rate', fontweight='bold')
axes[1].set_title('KNN Models', fontsize=12, fontweight='bold')
axes[1].legend()
axes[1].grid(alpha=0.3)

# SVM
for kernel in ['linear', 'poly', 'rbf', 'sigmoid']:
    model_name = f'SVM ({kernel})'
    if model_name in results and results[model_name]['pred_proba'] is not None:
        fpr, tpr, _ = roc_curve(y_test, results[model_name]['pred_proba'])
        roc_auc = auc(fpr, tpr)
        axes[2].plot(fpr, tpr, label=f'{kernel} (AUC = {roc_auc:.3f})', linewidth=2)

axes[2].plot([0, 1], [0, 1], 'k--', linewidth=2)
axes[2].set_xlabel('False Positive Rate', fontweight='bold')
axes[2].set_ylabel('True Positive Rate', fontweight='bold')
axes[2].set_title('SVM Models', fontsize=12, fontweight='bold')
axes[2].legend()
axes[2].grid(alpha=0.3)

plt.tight_layout()
plt.show()

#  K-FOLD CROSS VALIDATION (K=5)

print(" K-FOLD CROSS VALIDATION (K=5)")

kfold = KFold(n_splits=5, shuffle=True, random_state=42)
cv_results = {}

# Naïve Bayes
print("\nNaïve Bayes Models:")
print("-" * 70)
for name, model_class in [('Gaussian NB', GaussianNB()),
                          ('Multinomial NB', MultinomialNB()),
                          ('Bernoulli NB', BernoulliNB())]:
    scores = cross_val_score(model_class, X_train, y_train, cv=kfold, scoring='accuracy')
    cv_results[name] = scores
    print(f"{name:20s} - Mean: {scores.mean():.4f} (+/- {scores.std():.4f})")
    print(f"{'':20s}   Fold Scores: {[f'{s:.4f}' for s in scores]}")

# KNN
print("\nKNN Models:")
print("-" * 70)
for k in [3, 5, 7, 9, 11]:
    model = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(model, X_train_scaled, y_train, cv=kfold, scoring='accuracy')
    name = f'KNN (k={k})'
    cv_results[name] = scores
    print(f"{name:20s} - Mean: {scores.mean():.4f} (+/- {scores.std():.4f})")
    print(f"{'':20s}   Fold Scores: {[f'{s:.4f}' for s in scores]}")

# SVM
print("\nSVM Models:")
print("-" * 70)
for kernel in ['linear', 'rbf', 'poly', 'sigmoid']:
    if kernel == 'poly':
        model = SVC(kernel=kernel, degree=3, random_state=42)
    else:
        model = SVC(kernel=kernel, random_state=42)
    scores = cross_val_score(model, X_train_scaled, y_train, cv=kfold, scoring='accuracy')
    name = f'SVM ({kernel})'
    cv_results[name] = scores
    print(f"{name:20s} - Mean: {scores.mean():.4f} (+/- {scores.std():.4f})")
    print(f"{'':20s}   Fold Scores: {[f'{s:.4f}' for s in scores]}")

# Visualize cross-validation results
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Naïve Bayes
nb_models = ['Gaussian NB', 'Multinomial NB', 'Bernoulli NB']
nb_scores = [cv_results[m] for m in nb_models]
bp1 = axes[0].boxplot(nb_scores, labels=nb_models, patch_artist=True)
for patch in bp1['boxes']:
    patch.set_facecolor('#3b82f6')
axes[0].set_title('Naïve Bayes - 5-Fold CV', fontsize=12, fontweight='bold')
axes[0].set_ylabel('Accuracy', fontweight='bold')
axes[0].grid(axis='y', alpha=0.3)
axes[0].tick_params(axis='x', rotation=15)

# KNN
knn_models = [f'KNN (k={k})' for k in [3, 5, 7, 9, 11]]
knn_scores = [cv_results[m] for m in knn_models]
bp2 = axes[1].boxplot(knn_scores, labels=[f'k={k}' for k in [3, 5, 7, 9, 11]],
                       patch_artist=True)
for patch in bp2['boxes']:
    patch.set_facecolor('#10b981')
axes[1].set_title('KNN - 5-Fold CV', fontsize=12, fontweight='bold')
axes[1].set_ylabel('Accuracy', fontweight='bold')
axes[1].grid(axis='y', alpha=0.3)

# SVM
svm_models = [f'SVM ({k})' for k in ['linear', 'rbf', 'poly', 'sigmoid']]
svm_scores = [cv_results[m] for m in svm_models]
bp3 = axes[2].boxplot(svm_scores, labels=['linear', 'rbf', 'poly', 'sigmoid'],
                       patch_artist=True)
for patch in bp3['boxes']:
    patch.set_facecolor('#f59e0b')
axes[2].set_title('SVM - 5-Fold CV', fontsize=12, fontweight='bold')
axes[2].set_ylabel('Accuracy', fontweight='bold')
axes[2].grid(axis='y', alpha=0.3)
axes[2].tick_params(axis='x', rotation=15)

plt.tight_layout()
plt.show()

# COMPREHENSIVE MODEL COMPARISON

# Create comparison dataframe
comparison_data = []
for model_name, result in results.items():
    comparison_data.append({
        'Model': model_name,
        'Accuracy': result['accuracy'],
        'Precision': result['precision'],
        'Recall': result['recall'],
        'F1-Score': result['f1_score']
    })

comparison_df = pd.DataFrame(comparison_data)
comparison_df = comparison_df.sort_values('F1-Score', ascending=False)

print("\nTest Set Performance (sorted by F1-Score):")
print("=" * 80)
print(comparison_df.to_string(index=False))

# Visualize comparison
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
colors_map = {'Accuracy': '#3b82f6', 'Precision': '#10b981',
              'Recall': '#f59e0b', 'F1-Score': '#ef4444'}

for idx, metric in enumerate(metrics):
    ax = axes[idx // 2, idx % 2]
    top_10 = comparison_df.nlargest(10, metric)

    bars = ax.barh(range(len(top_10)), top_10[metric].values, color=colors_map[metric])
    ax.set_yticks(range(len(top_10)))
    ax.set_yticklabels(top_10['Model'].values)
    ax.set_xlabel(metric, fontweight='bold')
    ax.set_title(f'Top 10 Models by {metric}', fontsize=12, fontweight='bold')
    ax.grid(axis='x', alpha=0.3)
    ax.invert_yaxis()

    # Add value labels
    for i, (bar, val) in enumerate(zip(bars, top_10[metric].values)):
        ax.text(val + 0.01, i, f'{val:.3f}', va='center', fontweight='bold')

plt.tight_layout()
plt.show()

# Model type comparison
model_types = {
    'Naïve Bayes': [m for m in results.keys() if 'NB' in m],
    'KNN': [m for m in results.keys() if 'KNN' in m and 'k=' in m],
    'SVM': [m for m in results.keys() if 'SVM' in m]
}

type_comparison = []
for model_type, models in model_types.items():
    if models:
        type_metrics = {
            'Accuracy': np.mean([results[m]['accuracy'] for m in models]),
            'Precision': np.mean([results[m]['precision'] for m in models]),
            'Recall': np.mean([results[m]['recall'] for m in models]),
            'F1-Score': np.mean([results[m]['f1_score'] for m in models])
        }
        type_comparison.append({'Model Type': model_type, **type_metrics})

type_df = pd.DataFrame(type_comparison)

print("\n" + "=" * 80)
print("Average Performance by Model Type:")
print("=" * 80)
print(type_df.to_string(index=False))

# Radar chart for model type comparison
from math import pi

fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))

angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
angles += angles[:1]

colors_radar = ['#3b82f6', '#10b981', '#f59e0b']

for idx, row in type_df.iterrows():
    values = [row[m] for m in metrics]
    values += values[:1]
    ax.plot(angles, values, 'o-', linewidth=2, label=row['Model Type'], color=colors_radar[idx])
    ax.fill(angles, values, alpha=0.15, color=colors_radar[idx])

ax.set_xticks(angles[:-1])
ax.set_xticklabels(metrics, fontweight='bold')
ax.set_ylim(0, 1)
ax.set_title('Model Type Comparison - All Metrics', fontsize=14, fontweight='bold', pad=20)
ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))
ax.grid(True)

plt.tight_layout()
plt.show()

#  FINAL OBSERVATIONS AND RECOMMENDATIONS

best_model = comparison_df.iloc[0]
print(f"\n BEST OVERALL MODEL: {best_model['Model']}")
print("-" * 80)
print(f"   Accuracy:  {best_model['Accuracy']:.4f}")
print(f"   Precision: {best_model['Precision']:.4f}")
print(f"   Recall:    {best_model['Recall']:.4f}")
print(f"   F1-Score:  {best_model['F1-Score']:.4f}")

print("\n KEY OBSERVATIONS:")
print("-" * 80)

# Best in each category
for metric in metrics:
    best = comparison_df.nlargest(1, metric).iloc[0]
    print(f"   Best {metric:12s}: {best['Model']:25s} ({best[metric]:.4f})")